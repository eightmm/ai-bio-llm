# Designing a Multimodal AI Framework for Predicting Cellular Aging and Disease Progression from TERRA Modification Patterns

Cellular aging represents one of the most fundamental biological processes underlying age-related disease, yet identifying early molecular signatures that reliably predict disease progression remains a critical challenge in biomedical research. Emerging evidence demonstrates that telomeric repeat-containing RNA (TERRA) and its modification landscape, particularly modifications such as N6-methyladenosine (m6A), pseudouridine (ψ), and 5-methylcytosine (m5C), constitute promising indicators of cellular senescence, genome instability, and pathogenic trajectories. This report presents a comprehensive framework for integrating heterogeneous TERRA-related multi-omics and imaging data through advanced artificial intelligence to predict cellular aging status and disease progression probability, while simultaneously quantifying the functional contribution of individual RNA modification types and linking molecular features to mechanistic biological pathways. The proposed integrated framework encompasses three interconnected modules that progress from raw data heterogeneity to interpretable risk predictions with explicit biological grounding.

## Module A: Multimodal Data Integration and Harmonization for TERRA Biology

### Data Modality Specification and Preprocessing Strategy

The foundation of any predictive framework rests upon the quality and comprehensiveness of integrated data. TERRA modification patterns exist at the nexus of multiple biological layers—transcriptional regulation, post-transcriptional modification, chromatin architecture, and three-dimensional nuclear organization—requiring simultaneous capture across complementary measurement technologies. The proposed framework integrates five primary data modalities, each addressing distinct facets of TERRA biology and cellular aging status.

Transcriptomic data serves as the first major modality and encompasses both bulk RNA-sequencing and single-cell RNA-sequencing (scRNA-seq) measurements. Bulk RNA-seq provides transcriptome-wide expression quantification across tissue populations, capturing aggregate changes in telomere maintenance genes, DNA damage response (DDR) pathways, and senescence-associated genes. Raw read counts from bulk RNA-seq require normalization using either transcripts per million (TPM) or DESeq2 variance-stabilizing transformation (VST) to account for compositional effects and sequencing depth variation[9]. Crucially, unwanted variation from technical sources—including batch effects, library preparation artifacts, and sample processing variations—must be removed prior to downstream analysis using methods such as RUVSeq or SVA (Surrogate Variable Analysis) to recover true biological signal[41]. These normalized and batch-corrected bulk expression matrices form the basis for differential expression analysis relative to aging status or disease progression phenotypes.

Single-cell RNA-seq captures transcriptional heterogeneity within populations, revealing cell-type-specific responses to telomere stress and identifying which cellular compartments drive aging-associated expression changes. However, scRNA-seq data presents computational challenges including batch effects between samples, dropout events, and technical zeros. The scvi-tools suite, particularly the scVI (single-cell Variational Inference) framework, offers a principled probabilistic approach to simultaneously handle dimensionality reduction, batch correction, and denoising[12]. The scVI model leverages a variational autoencoder (VAE) architecture that explicitly models the zero-inflated nature of droplet-based scRNA-seq counts while learning a shared low-dimensional latent space across batches. After scVI training, cell-type annotation can be performed through reference-mapping strategies, enabling interpretation of which cell types undergo senescence or accumulate telomeric stress. Critically, scVI's integration of multiple scRNA-seq batches into a unified latent space addresses a primary limitation of earlier batch-correction approaches that were primarily designed for bulk data and may not capture complex non-linear relationships inherent in single-cell measurements.

RNA modification data represents the second major modality and directly measures TERRA modification patterns that drive the predictive signal. m6A modifications on TERRA are predominantly deposited co-transcriptionally by the METTL3-METTL14-WTAP methyltransferase complex and recognized by reader proteins such as YTHDF2 and YTHDC1[1]. Experimental identification of transcriptome-wide m6A sites can be achieved through multiple complementary approaches, each with distinct advantages and limitations. MeRIP-seq (methylated RNA immunoprecipitation followed by sequencing) uses antibodies to pull down m6A-modified RNA and requires paired input controls for normalization; however, antibody specificity and variation between experiments can introduce batch effects and false positives. miCLIP (m6A crosslinking and immunoprecipitation) provides single-nucleotide resolution of m6A sites through ultraviolet crosslinking but is technically demanding and labor-intensive. Direct RNA sequencing via nanopore technology offers an emerging orthogonal approach that detects modifications through altered current intensities as modified nucleotides traverse sequencing pores; supervised methods such as m6Anet and EpiNano can infer m6A sites from nanopore raw signals by comparing to reference or training data obtained from miCLIP or m6ACE-seq experiments[7]. The computational preprocessing of RNA modification data must include peak/site calling with appropriate statistical thresholds, assignment of modifications to genomic coordinates (particularly TERRA loci and subtelomeric repeat regions), quantification of peak intensities as a proxy for modification abundance, and calculation of per-sample or per-condition modification rates. For TERRA-specific analysis, modification sites should be aligned to telomeric repeat sequences and subtelomeric genes known to regulate telomere biology, creating a modification feature matrix where rows represent genomic positions or transcript isoforms and columns represent samples.

ChIP-seq (chromatin immunoprecipitation followed by sequencing) provides spatial information on telomere-associated protein binding and histone modifications that regulate TERRA transcription and R-loop formation. ChIP-seq data targeting telomere-binding factors (such as TRF1, TRF2, POT1) or histone marks associated with transcriptionally active or repressed regions (H3K4me3, H3K27me3, H3K27ac) must undergo rigorous quality control and normalization. Sequencing depth normalization using counts per million (CPM) or relative genome coverage (RPGC) corrects for library size variation, while peak calling algorithms such as MACS2 or HOMER identify regions of statistically significant enrichment above background[35]. Quality metrics including Fragment in Peaks (FRiP) ratio, cross-correlation analysis (NSC/RSC), and visual inspection of enrichment patterns assess data quality before downstream analysis. Peak counts and signal intensity measures over telomere-proximal genomic regions are extracted to create feature vectors representing chromatin state and transcription factor occupancy patterns relevant to TERRA regulation.

Imaging data constitutes the fourth modality and captures morphological and spatial aspects of telomere organization and TERRA localization. Telomere fluorescence in situ hybridization (FISH) uses fluorescently labeled probes complementary to TTAGGG repeats to visualize telomere foci within nuclei; image analysis yields quantitative measurements including the number of discrete telomere foci per nucleus, mean and median telomere fluorescence intensity, spatial distribution patterns, and inferred telomere length proxies based on signal intensity correlations[16]. TERRA RNA-FISH uses probes targeting TERRA transcripts and reveals whether TERRA molecules are actively transcribed and localized at telomeres or dispersed throughout the nucleus, with differential TERRA localization implicated in alternative lengthening of telomeres (ALT) and aging phenotypes. Super-resolution imaging techniques such as stimulated emission depletion (STED) or structured illumination microscopy (SIM) resolve telomere architecture at nanometer scales, revealing sub-nuclear organization of telomeric chromatin and R-loop structures that are invisible to conventional confocal microscopy. Automated cell segmentation using Cellpose, a deep learning framework for instance-based segmentation that generalizes across imaging modalities and cell morphologies[17][20], segments individual nuclei from DAPI staining and telomeric foci from FISH channels. Downstream feature extraction on segmented images generates morphological descriptors including foci count, intensity distributions, circularity metrics, and spatial clustering coefficients. These handcrafted features can be complemented with learned embeddings from foundation vision models such as DINOv2 or Vision Transformers (ViT) fine-tuned on the specific microscopy dataset, capturing higher-order spatial and textural patterns that manual feature extraction may miss.

### Cross-Modal Alignment and Fusion Strategy

After modality-specific preprocessing, the heterogeneous data representations must be aligned into a unified reference space suitable for joint analysis and prediction. The proposed framework employs a deep generative multimodal integration approach as the primary strategy, motivated by the need to handle missing modalities (some samples may lack complete measurements across all five modalities), to model non-linear cross-modal relationships, and to generate probabilistic latent representations suitable for downstream predictive modeling.

The core integration method builds upon the MultiVI framework from scvi-tools, originally developed for integrating scRNA-seq and scATAC-seq data[2]. MultiVI uses a variational autoencoder with modality-specific encoder and decoder networks, allowing each modality to have distinct distributional assumptions (e.g., zero-inflated for RNA-seq, binary for chromatin accessibility, continuous for modifications) while sharing a joint latent variable that captures the underlying biological state. However, the standard MultiVI is optimized for two modalities; the proposed framework extends this conceptually to handle five modalities through a product-of-experts (PoE) VAE architecture. In this design, each modality k (bulk RNA, scRNA, modifications, ChIP-seq, imaging) has a dedicated encoder network \(\text{Enc}_k(x_k) \rightarrow q_k(\mathbf{z}|x_k)\) that maps observed data to a posterior distribution over latent variables. The joint posterior is constructed as:

\[
q(\mathbf{z}|x_1, \ldots, x_5) \propto \prod_{k=1}^{5} q_k(\mathbf{z}|x_k) \, p(\mathbf{z})
\]

where \(p(\mathbf{z})\) is a standard normal prior. This product-of-experts formulation naturally handles missing modalities by simply excluding missing factors from the product. The VAE training objective combines modality-specific reconstruction losses—binary cross-entropy for binary ChIP-seq peaks, zero-inflated negative binomial for RNA-seq data, mean-squared error for real-valued modification intensities—with a shared Kullback-Leibler divergence term:

\[
\mathcal{L} = \sum_k \mathcal{L}_{\text{recon}}^{(k)} + \beta \, \text{KL}(q(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z}))
\]

The parameter \(\beta\) can be annealed during training to encourage the model to prioritize reconstruction early and gradually introduce the regularization, a strategy known as beta-VAE that often improves latent space quality. Upon convergence, the learned latent variable \(\mathbf{z}_{\text{shared}} \in \mathbb{R}^{d_z}\) (typically of dimension 32–128) captures the essential biological state combining information across all modalities. Additionally, the framework retains modality-specific latent variables \(\mathbf{z}_k\) from each encoder pathway, enabling later analysis of modality-specific contributions without losing information unique to individual measurement types.

A critical consideration in TERRA modification biology is preserving separate feature channels for each modification type (m6A, ψ, m5C) to enable modification-specific interpretation and attribution analysis. Rather than simply concatenating modification features into a single modification data matrix, the framework maintains structurally separate feature blocks for each modification type. During VAE integration, these modification-type blocks are processed through separate encoder sub-networks with shared representations in later layers, allowing the framework to learn both modification-type-specific patterns and higher-order cross-type interactions (e.g., cooperative or antagonistic relationships between m6A and m5C at the same loci). This architectural choice is justified by emerging evidence that modification cross-talk at individual RNA molecules modulates function; for instance, m6A and m5C modifications may compete for the same cytosine residues or cooperatively regulate R-loop processing[1].

An important alternative design choice is classical concatenation followed by batch correction using ComBat or Harmony, followed by dimensionality reduction with PCA or UMAP. This approach is simpler computationally but has significant limitations. Concatenation ignores the distributional heterogeneity between modalities (e.g., treating continuous imaging embeddings identically to count data violates statistical assumptions), ComBat may overcorrect when modalities have genuine biological differences in batch structure, and UMAP/PCA provide only deterministic embeddings without uncertainty quantification or principled handling of missing data[12][18]. The deep generative approach is therefore justified despite increased computational cost.

A second alternative is MOFA+ (Multi-Omics Factor Analysis Plus), a Bayesian factor analysis framework that decomposes high-dimensional multi-omics data into latent factors[8]. MOFA+ has demonstrated excellent performance on omics integration but has limitations for raw imaging embeddings (which are inherently high-dimensional and complex) and may underfit non-linear relationships between modification types and downstream phenotypes. MOFA+ is therefore retained as a computational baseline for robustness assessment but is not selected as the primary integrator.

### Module A Outputs and Downstream Specification

The outputs from Module A integration consist of three complementary representations, each tailored for specific downstream analyses. First, the **unified latent embedding** \(\mathbf{z}_{\text{shared}}\) provides a sample-level summary of biological state capturing the integrated aging/disease signature across all modalities. Each sample receives a single low-dimensional vector representing its position in the learned latent space, with geometric proximity in latent space suggesting phenotypic similarity. This shared embedding forms the primary input to the Module B predictive model.

Second, **modality-specific latent embeddings** \(\mathbf{z}_k\) for each modality k are retained for downstream attribution analysis. By comparing how the shared embedding relates to each modality-specific embedding, the framework can quantify whether particular modalities drive the aging/disease signal. Additionally, if certain samples lack measurements in a particular modality, the learned posterior mean \(\mathbb{E}[\mathbf{z}_k | x_{\text{observed}}]\) provides a probabilistic imputation of that missing modality, enabling complete-data inference for predictive models without removing incompletely-measured samples.

Third, the framework extracts **engineering-optimized feature matrices** for each modification type (m6A, ψ, m5C) before or after VAE integration. Rather than relying solely on the compressed latent representation, these feature matrices retain information about specific high-impact modification sites or peaks that may be crucial for mechanistic interpretation. For each modification type, features include per-site or per-peak modification intensity, local sequencing depth, sequence context motif information (e.g., m6A consensus DRACH motifs), and genomic position relative to TERRA loci. These features are crucial for Module C attribution analysis and provide granular resolution on which specific modifications drive predictions.

The unified representation additionally includes **harmonized metadata covariates** such as sample batch, sequencing technology, cell type composition (from scRNA-seq reference mapping), and clinical metadata (age, sex, disease status, follow-up time) organized into a consistent dataframe. These covariates facilitate downstream stratified analyses and enable testing whether predictions generalize across batch, technology, and demographic subgroups.

A final output consists of **quality control metrics** assessing integration success. Metrics include silhouette scores or trustworthiness measures comparing local neighborhoods in latent space to original high-dimensional space, reconstruction error per modality (indicating whether all modalities are adequately captured), and cross-validation likelihood on held-out samples (indicating whether the model generalizes). These QC metrics are essential for determining whether integration was successful and for identifying potentially problematic samples or modalities requiring additional preprocessing.

## Module B: Multimodal Predictive Model for Aging and Disease Risk

### Primary Architectural Design: Multimodal Transformer with Modification-Specific Tokens

The transformed and integrated data from Module A must be mapped through a predictive model that learns associations between TERRA modification patterns and aging/disease outcomes while preserving interpretability. The proposed framework employs a **multimodal Transformer architecture** as the primary predictor, justified by three key properties: (i) Transformers' self-attention mechanisms enable learning of arbitrary pairwise interactions between modalities and modification types without explicit prior specification; (ii) modality-specific tokenization preserves information about which modifications drove predictions, enabling attribution analysis; and (iii) Transformers naturally handle variable-length sequences and incomplete data through masking, supporting samples with missing modalities. This design directly parallels recent successes in medical image analysis where Vision Transformers fused with text embeddings have outperformed traditional approaches[4], and extends naturally to genomic applications where attention mechanisms have demonstrated efficacy in predicting transcription factor binding sites[24].

The Transformer model processes inputs as a sequence of tokens, each with an associated embedding. In the multimodal TERRA context, tokens are constructed as follows. The shared latent embedding \(\mathbf{z}_{\text{shared}}\) from Module A is passed through a learnable projection layer to generate an initial "context token" that encodes the integrated biological state. Additionally, modification-type-specific tokens are created by dividing the engineered modification feature matrix into separate token groups for m6A, ψ, and m5C, with each token representing either a single modification site or an aggregated feature within a genomic bin. Analogously, imaging features (telomere count, morphology metrics, spatial clustering) are concatenated to form imaging tokens, ChIP-seq signals generate occupancy tokens, and bulk/scRNA expression changes generate expression tokens. Each token is embedded into a high-dimensional space (e.g., 256 or 512 dimensions) using learnable embedding layers, and absolute positional encodings are added to preserve sequence order information.

The core Transformer encoder processes these tokenized inputs through multi-head self-attention layers. Each attention head learns to weight the contribution of all tokens when computing a representation for any given token, allowing the model to discover dependencies such as "modification sites in certain sequence contexts tend to co-occur with high expression of telomere maintenance genes" or "specific morphology patterns on imaging correlate with modification abundance." Cross-attention layers optionally fuse information from different modality-specific tokens, enabling the model to learn how modifications interact with expression changes and morphological features. The Transformer encoder typically contains 4–8 attention layers with 8–16 parallel attention heads per layer, configured using standard attention mechanisms with layer normalization and residual connections to stabilize training on large heterogeneous data[24].

To generate predictions, the encoder output is passed to task-specific prediction heads. For **classification of aging status**, a classification head with sigmoid (binary) or softmax (multi-class) activation produces probabilities for membership in discrete aging classes (e.g., young, intermediate, aged). For **disease progression probability**, a similar classification head maps learned representations to disease/no-disease probabilities. For **time-to-event prediction** (modeling the latency until disease onset or functional decline), the framework employs a survival head based on the DeepSurv architecture, which learns a non-linear transformation of covariates to predict a proportional hazards score:

\[
h(t|\mathbf{x}) = h_0(t) \exp(f_{\theta}(\mathbf{x}))
\]

where \(f_{\theta}\) is the learned Transformer representation and \(h_0(t)\) is a non-parametric baseline hazard estimated from the training data[11][14]. Alternatively, discrete-time hazard modeling can be employed using Breslow-type estimators at observed event times, which better handles tied event times and administrative censoring common in clinical datasets.

**Calibration is explicitly incorporated** into Module B to ensure that predicted probabilities accurately reflect true event probabilities. Overconfidence in modern neural networks is well-documented; temperature scaling, a simple post-processing technique, rescales logit scores before softmax by a scalar temperature parameter \(T\) such that predicted probabilities become more conservative and better-calibrated[32]. Temperature parameters are estimated on a held-out validation set by minimizing negative log-likelihood and are typically in the range 1.5–3.0 for well-trained deep networks[32]. This ensures that a predicted probability of 0.7 genuinely reflects approximately 70% empirical frequency of the event.

**Regularization and robustness** are implemented through multiple mechanisms. Dropout layers after each attention head (with dropout rates typically 0.1–0.2) prevent co-adaptation of units and improve generalization. During training, modality-specific dropout randomly masks entire modalities with a small probability (e.g., 5%), forcing the model to learn robust predictions that do not depend excessively on any single modality—directly simulating the missing-data scenarios encountered in practice. Monte Carlo dropout, where dropout is retained during inference and multiple forward passes are performed, enables uncertainty quantification via ensemble-like averaging. Deep ensembles, where multiple Transformer models are trained with different random initializations, provide complementary uncertainty estimates and often outperform single models.

The **training objective** combines multiple losses to optimize both prediction accuracy and auxiliary objectives that improve robustness. The primary classification loss uses focal loss or weighted cross-entropy to handle potential class imbalance (aging or disease may be rarer than controls). For survival tasks, the loss is the negative log partial likelihood from the Cox model applied to the predicted risk scores. An auxiliary alignment loss encourages the learned shared latent embedding from Module A to remain close to its Module B representation, preventing the predictive model from drifting far from the integrated baseline. A contrastive loss encourages samples from the same aging/disease class to have similar representations in latent space while pushing different classes apart, similar to supervised contrastive learning. The combined loss is:

\[
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{pred}} + \lambda_1 \mathcal{L}_{\text{align}} + \lambda_2 \mathcal{L}_{\text{contrast}}
\]

where \(\lambda_1, \lambda_2\) are hyperparameters balancing the objectives. Multi-task learning (joint optimization of classification and survival, if applicable) has been demonstrated to improve generalization by providing implicit regularization through shared intermediate layers.

### Biological Justification and Alternative Architectures

The proposed multimodal Transformer architecture is grounded in TERRA modification biology through several specific considerations. First, m6A, ψ, and m5C modifications regulate R-loop formation and resolution at telomeres, and R-loops themselves modulate telomere maintenance, senescence, and alternative lengthening of telomeres (ALT) pathways[1][5]. A model that simultaneously processes modifications, ChIP-seq evidence of chromatin state, and expression changes can learn these complex regulatory interactions; Transformer cross-attention is particularly suited for discovering such dependencies. Second, TERRA localization and processing are spatially heterogeneous within the nucleus, with different modification patterns in telomere-proximal versus extranuclear TERRA; imaging data capturing this spatial information is crucial, and Transformers handle image-derived embeddings naturally. Third, emerging evidence from systems aging research demonstrates that composite aging clocks integrating multiple data types outperform single-modality clocks[8], motivating multimodal integration at the model level rather than post-hoc meta-analysis.

Alternative model architectures were considered and rejected. **Classical machine learning models (Elastic Net, XGBoost, Random Forests)** on concatenated flattened feature vectors represent a simpler approach and serve as computational baselines. However, these methods lack capacity to learn non-linear cross-modal interactions and perform poorly when input dimensionality is high (as with image embeddings, which may be 1000–10000 dimensions); additionally, they do not naturally incorporate uncertainty quantification or structured handling of missing modalities. **Graph Neural Networks (GNNs)** over genomic loci or RNA-binding protein (RBP)–RNA interaction networks are biologically appealing, as telomere biology involves complex networks of protein-DNA and protein-RNA interactions; however, GNNs require high-quality graph priors that may not be fully reliable and are brittle to graph misspecification. GNNs could be valuable as an extension or alternative head on top of shared embeddings from Module A but are not selected as the primary architecture. **Mixture-of-Experts (MoE) architectures**, where multiple expert networks specialize in different aspects of the data, are increasingly popular and could allow separate experts for modification patterns, imaging, and expression[42]. However, MoE models suffer from training instability and load-balancing challenges, and the overhead of maintaining multiple experts is not justified for the current framework size; MoE could be considered if the model is scaled to much larger datasets.

### Module B Outputs and Feeding into Module C

The predictive model produces **primary outputs** consisting of sample-level predictions: aging risk score (continuous), aging class probabilities (per class), disease progression probability (continuous), and time-to-event risk scores with associated confidence intervals (from ensemble averaging or MC dropout). These predictions are the direct answer to the clinical/biological question posed.

Additionally, intermediate representations are saved for attribution analysis: **(i) attention weight matrices** from each Transformer attention head, showing which tokens attended to which other tokens; **(ii) gradient-based attribution** computed as the gradient of the output prediction with respect to input token embeddings, enabling identification of input features most influential for predictions; **(iii) intermediate layer activations** before and after key Transformer blocks, allowing insertion of ablation studies or intervention experiments; and **(iv) gating weights or routing probabilities** if ensemble or mixture-of-experts elements are included.

## Module C: Interpretation, Mechanistic Explainability, and Rigorous Evaluation

### Quantifying and Visualizing Individual Modification Type Contributions

The core objective of Module C is to answer the mechanistic question: **which RNA modifications drive aging/disease risk predictions, and how do they exert their effects through biological pathways?** This requires systematic quantification of modification-type importance, sensitive attribution methods, and integration with biological domain knowledge.

The first approach, **modality-level ablation**, involves systematically removing each modification type's feature block and retraining or re-evaluating the Module B predictive model. Specifically, three ablated models are trained: (i) baseline model with all modifications, (ii) m6A-only ablated model (set m6A features to zero or exclude from tokenization), (iii) ψ-only ablated model, and (iv) m5C-only ablated model. Performance metrics (AUROC for classification, C-index for survival, calibration error) are computed on the same test set, and delta-metrics (change from baseline) quantify each modification's contribution. A modification with delta-AUROC = 0.08 (8% decrease in AUROC when ablated) is deemed more important than one with delta-AUROC = 0.02. This ablation provides causal-ish evidence at the modification-type block level; limitations include potential interactions (the effect of removing m6A may depend on whether m5C is present) but the approach remains interpretable and actionable. Ablation studies are inherently model-agnostic and remain among the most reliable approaches for feature importance quantification[27][30].

The second approach, **gradient-based attribution**, uses Integrated Gradients (IG) or DeepSHAP to assign importance scores to individual modification sites or features[28][31]. Integrated Gradients computes the gradient of the output prediction with respect to input features along a path from a reference (e.g., zero or mean values) to observed values:

\[
\text{Attr}_i(x) = (x_i - x'_i) \int_{\alpha=0}^{1} \frac{\partial f(x' + \alpha(x - x'))}{\partial x_i} d\alpha
\]

In practice, the integral is approximated via Riemann summation over 50–100 interpolation steps. Attributions are signed, with positive values indicating the feature pushes the prediction toward high risk and negative values toward low risk. For TERRA modifications, IG applied to the modification feature tokens produces per-site importance: which specific m6A peaks, ψ modifications, and m5C sites most strongly influence aging/disease predictions? This fine-grained resolution enables ranking individual modification sites, e.g., "the m6A site at position 146,582 on chromosome 5q (subtelomeric region) contributes 12% of the overall TERRA m6A signal driving high-risk predictions." DeepSHAP extends this by computing attributions relative to a baseline distribution rather than a single reference, potentially providing more robust estimates[31]. An important distinction is that IG/DeepSHAP attributions reflect the model's learned associations and do not necessarily imply biological causation; however, when coupled with orthogonal biological validation, they provide strong hypotheses for experimental follow-up.

Attention weights, while appealing for interpretability, are not selected as the primary attribution method because attention does not guarantee attribution—a token may receive low attention yet substantially influence downstream predictions through non-linear interactions. Attention weights are retained as supportive evidence, visualized through "attention rollout" (recursively propagating attention matrices across layers), but are not trusted as sole attribution evidence[24].

### Mechanistic Interpretability to Biological Pathways

Attribution of importance to individual modification sites or features is insufficient without connecting these molecular features to higher-order biological pathways and cellular processes. Module C therefore implements a **pathway enrichment analysis pipeline** linking high-importance modifications to biological function. The workflow is as follows. High-attribution modification sites (identified through ablation and IG analysis) are mapped to their genomic coordinates and associated genes or transcripts. If m6A sites are enriched within the 3' UTRs of specific genes (a known regulatory region for m6A function), those genes are extracted. Standard pathway enrichment methods such as Gene Set Enrichment Analysis (GSEA) or Fisher's exact test via g:Profiler are applied to test whether the list of high-attribution genes is enriched for specific biological pathways relative to genome-wide expectation[23][26].

Curated gene sets of particular relevance to TERRA and cellular aging include **telomere maintenance pathways** (KEGG hsa04915, Reactome "Telomere Maintenance"), **DNA damage response and checkpoint pathways** (TP53 targets, ATM/ATR signaling), **replication stress pathways**, **chromatin remodeling complexes**, and **aging-hallmark gene sets** (from MSigDB or custom curation based on literature). For instance, if high-attribution m6A sites are predominantly within transcripts of genes involved in homologous recombination (which is engaged by TERRA R-loops), this implicates m6A in regulating recombination-dependent telomere maintenance. Similarly, if ψ modifications are enriched on transcripts encoding RNA-binding proteins that process R-loops, this suggests ψ functions to regulate R-loop resolution.

A more sophisticated approach is **mediation analysis**, which decomposes the predictive importance of a modification-type feature into (i) direct effects (the modification independently influences prediction) and (ii) indirect effects mediated through downstream gene expression or chromatin state changes. A two-stage model formulation tests whether modifying m6A feature inputs primarily affects the outcome through changes in expression of specific genes (e.g., telomere-binding proteins or DDR effectors). This mediation analysis requires fitting conditional models and computing natural direct and indirect effects, but provides mechanistic insight into which downstream processes are perturbed by TERRA modifications.

### Visualization and Communication Strategies

Module C outputs include multiple visualization modalities tailored for different audiences. For **modification-type importance**, stacked bar plots show the aggregate contribution of each modification type (m6A, ψ, m5C) to predictions, either globally (averaged across samples) or stratified by aging class or disease status. Violin plots compare the distribution of per-site attribution scores for each modification type, revealing whether m6A sites cluster in high-importance ranges or are more widely distributed. **Sample-level explanation cards** provide personalized interpretation for each patient or cell sample, listing the top 5–10 most important features (specific m6A peaks, morphology metrics, expression changes) that collectively explain why the model predicts high or low aging/disease risk for that individual.

For genomic context, **Manhattan-style tracks** display modification sites across the telomeric and subtelomeric genomic regions, with color and height encoding per-site attribution scores. Regions with high-importance modifications stand out visually, enabling rapid identification of genomic loci driving predictions. **Heatmaps** show co-occurrence patterns: which combinations of modifications are most predictive? Do high m6A abundance and low m5C co-segregate with high aging risk, or are they independent contributors? Such patterns inform hypotheses about potential cross-talk between modification types.

For biological interpretation, **enrichment plots** visualize pathway enrichment results, with pathway names on the y-axis and -log10(adjusted p-value) or normalized enrichment score on the x-axis. Pathways significantly enriched among high-attribution genes are highlighted, with associated gene lists and relevant literature citations included in supplementary materials. These visualizations are designed to be publication-quality and suitable for both specialist and general scientific audiences.

### Rigorous Evaluation Protocol

The predictive framework must be evaluated on held-out data never used during model training or hyperparameter selection, according to best practices in machine learning and clinical prediction modeling. The **data splitting strategy** uses patient-level splits (all samples from a single patient go to either training or test set, not both) to avoid inflating apparent performance through leakage of patient-specific confounders. If longitudinal follow-up is available (with samples from the same individual at multiple time points), temporal splits are employed: train on earlier time points, test on later time points, reflecting the true prospective use case. External validation on independent cohorts or datasets is the gold standard and is performed if available; such validation tests whether learned patterns generalize across batch, sequencing technology, and patient populations.

**Classification metrics** include area under the receiver operating characteristic curve (AUROC), area under the precision-recall curve (AUPRC), F1 score, sensitivity, specificity, and balanced accuracy (critical if aging classes are imbalanced). **Survival metrics** include the concordance index (C-index), which measures the rank-correlation between predicted risks and observed event times, ranging from 0.5 (random) to 1.0 (perfect), and integrated Brier score (IBS), which quantifies calibration-like accuracy across time. **Calibration metrics** assess whether predicted probabilities match empirical frequencies: Expected Calibration Error (ECE) computes the mean absolute difference between predicted and observed event frequencies in probability bins; well-calibrated models have ECE < 0.1. Calibration plots (confidence plots showing predicted vs. observed event rates) are constructed both marginally and stratified by age or subgroups.

An important but often-neglected metric is **decision curve analysis (DCA)**, which translates prediction accuracy into clinical utility[34][37]. DCA asks: at what threshold probability would a clinician recommend intervention, and does the predictive model identify patients for intervention with net benefit compared to strategies of "treat all" or "treat none"? A model may have high AUROC but provide no net clinical benefit if optimal thresholds correspond to unrealistic assumptions about the costs of false positives vs. false negatives. DCA curves plot net benefit against threshold probability and directly compare candidate models, providing decision-makers with intuitive guidance on whether to adopt the model clinically[34].

**Robustness and generalization** are tested through multiple stress tests. **Missing-modality stress tests** evaluate model performance when specific modalities (e.g., imaging data) are not available; models should gracefully degrade rather than crash or exhibit severe performance collapse[2]. **Batch and site generalization** tests performance on samples from different sequencing centers, imaging platforms, or institutions; good generalization suggests the model learned biological signal rather than batch artifacts. **Sensitivity to preprocessing choices** re-runs the entire pipeline with alternative normalization methods, batch correction algorithms, or hyperparameters, assessing whether key findings are robust to methodological choices or hinge on specific decisions.

**Statistical testing** accompanies all reported metrics. Bootstrap confidence intervals (resampling the test set with replacement 1000 times and recomputing metrics) quantify uncertainty around AUROC, C-index, and calibration measures. Permutation tests randomize the outcome labels and recompute metrics under the null hypothesis of no association; observed metrics far exceeding null permutation distributions provide strong evidence that associations are genuine rather than statistical artifacts. Between-group differences in model predictions (e.g., do different aging classes have significantly different predicted risks?) are tested using Mann-Whitney U tests (non-parametric) or Welch's t-tests, with multiple-testing correction applied if many comparisons are made.

### Feedback Loops and Iterative Model Refinement

Module C outputs inform an iterative refinement cycle that closes the loop back to Modules A and B. If pathway enrichment analysis reveals that high-importance modifications occur predominantly in specific sequence contexts or genomic regions not originally captured, Module A feature engineering can be refined to explicitly include contextual information (e.g., sequence k-mer composition surrounding modification sites, local chromatin structure). If ablation studies demonstrate that a particular modification type (e.g., ψ on TERRA) contributes minimal signal, resources can be redirected toward deeper profiling of other modifications or alternative data modalities. If attributions identify specific high-importance genomic loci, follow-up wet-lab experiments can be prioritized: targeted perturbation of METTL3-dependent m6A at those sites and measurement of consequent changes in telomere length, R-loop abundance, or senescence markers would validate the model's predictions.

Module C also generates **prioritized hypotheses for experimental validation**. For example, if the framework predicts that high m6A abundance on TERRA promotes ALT-dependent telomere lengthening, and if this prediction is consistent across aging classes and robust to batch variation, the hypothesis becomes actionable: knock out METTL3 specifically in ALT cells and measure telomere length, R-loop abundance, and cell proliferation; this experiment would test whether m6A-mediated telomere lengthening is METTL3-dependent. Similarly, if ψ modifications at specific TERRA loci emerge as protective against senescence, targeted pseudouridylation by modulating pseudouridine synthase activity could be tested in aged cells.

### Alternative Interpretation and Evaluation Choices

Several alternatives to the proposed interpretation strategy were evaluated and rejected or supplemented. **Relying solely on post-hoc attention for interpretation** is not selected as the primary approach because attention weights are permutation-invariant descriptors that do not directly encode causal influence; however, attention visualizations are retained as supporting evidence. **Pathway overrepresentation analysis without attribution** would ignore which specific features drive associations and could obscure important signals; attribution methods provide the necessary granularity. **Model-agnostic interpretability methods alone (e.g., LIME, kernel SHAP)** are more computationally expensive and may provide noisy estimates on high-dimensional data; gradient-based methods like IG are therefore selected as primary but SHAP is retained as a sensitivity analysis.

For evaluation, **using only AUROC without calibration or decision curve analysis** provides an incomplete picture; clinical utility depends on calibration, and decision curves translate statistical performance into actionable decision thresholds. **Evaluating on only a single held-out test set** without external validation or robustness testing overestimates generalization; the proposed protocol includes multiple forms of validation.

## Integration and Workflow Overview

The three modules form an integrated workflow that progresses from heterogeneous raw data through unified representation to interpretable risk predictions. Samples enter Module A with heterogeneous data: bulk RNA-seq count matrices, scRNA-seq count matrices, MeRIP-seq and miCLIP peaks, direct RNA-seq modification calls, ChIP-seq alignments, and microscopy images. These are separately preprocessed, QC-checked, and normalized according to modality-specific best practices. The product-of-experts VAE integration layer then creates unified representations: a shared latent vector capturing integrated aging/disease status, modality-specific vectors capturing modality-unique information, and retained feature matrices for each modification type.

Module B accepts these representations as input to the multimodal Transformer predictive model, which learns associations between integrated representations and clinical outcomes (aging status, disease progression, time-to-event). The Transformer generates predictions (risk scores, probabilities) and intermediate representations (attention weights, gradients, activations) that feed into Module C.

Module C interrogates the trained model to extract mechanistic insight. Ablation studies quantify modification-type importance; gradient-based attribution identifies high-impact individual sites; pathway enrichment connects molecular features to biological processes; and rigorous evaluation through multiple metrics, stress tests, and comparisons to baselines assesses prediction accuracy and generalization. Visualization and communication translate statistical results into actionable insights for clinicians and researchers. Feedback from Module C iteratively refines Modules A and B through refined feature engineering, model architecture adjustments, and prioritization of experimental follow-up.

## Biological Validation and Experimental Integration

For the framework to bridge the gap between computational prediction and biological understanding, close integration with wet-lab validation is essential. High-confidence predictions from the trained model can identify candidate TERRA modifications or genomic loci for targeted perturbation studies. For example, if m6A deposited by METTL3 on TERRA is predicted to promote telomere maintenance and is observed frequently in young, healthy samples but rarely in aged or diseased samples, CRISPR-based knockout of METTL3 in aged cells paired with measurement of telomere length, R-loop abundance (via DRIP-seq or GRID-seq), and senescence markers (p16 expression, SA-β-gal staining) would experimentally validate the computational prediction. Such experiments provide ground-truth assessment of whether the model's feature importance rankings correspond to biological causality and functional relevance.

Furthermore, if the framework identifies that specific sequence contexts or genomic regions harbor especially important modifications, targeted mutagenesis or synthetic biology approaches could be employed to test whether modifying these sites (e.g., disrupting DRACH motifs that recruit m6A writers, or inserting pseudouridine synthase recognition elements) alters aging-related phenotypes. Such bidirectional validation—computational predictions informing experiments, and experiments refining computational models—represents best practice for translating AI-based discovery into mechanistic understanding and therapeutic interventions.

## Conclusion and Future Perspectives

This comprehensive framework addresses the challenge of predicting cellular aging and disease progression from TERRA modification patterns through principled multimodal integration, transparent predictive modeling, and rigorous mechanistic interpretation. By maintaining modality-specific and modification-type-specific information throughout the analysis pipeline, the framework enables investigation of not only which samples are at high risk, but why—which specific modifications on which genomic loci drive risk through which biological pathways. The proposed deep generative integration strategy handles heterogeneous data distributions and missing modalities more robustly than classical alternatives; the multimodal Transformer architecture captures complex cross-modal interactions while preserving interpretability through structured tokenization; and the comprehensive evaluation protocol including ablations, attribution, pathway analysis, and decision curve analysis ensures that predictions are accurate, well-calibrated, generalizable, and clinically actionable.

The framework is designed with iterative refinement in mind, allowing feedback from mechanistic interpretation to successively improve data integration and predictive accuracy. Integration with wet-lab experimental validation transforms computational predictions into validated biological hypotheses and candidate therapeutic targets. As data availability increases and TERRA biology is elucidated further, the framework can be extended to incorporate additional data modalities (e.g., chromatin conformation capture data, metabolic measurements, circulating biomarkers), refined modification-specific processing pipelines (as direct RNA-seq and other long-read technologies mature), and increasingly sophisticated architectural choices (e.g., conditional generative models for counterfactual reasoning). The modular design ensures that technological advances can be incorporated without wholesale reengineering of the entire framework.

Ultimately, this framework serves as a template for the broader challenge of predicting aging and age-related disease risk from multi-omics data in a manner that is simultaneously predictively accurate and mechanistically interpretable, bridging the gap between statistical learning and biological discovery[8]. By grounding the AI framework in TERRA biology and explicitly modeling the role of RNA modifications in regulating telomere maintenance, DNA damage response, and cellular senescence, the approach aligns computational methods with emerging molecular understanding of aging, enabling both predictive translation to clinical settings and fundamental discovery of aging mechanisms.