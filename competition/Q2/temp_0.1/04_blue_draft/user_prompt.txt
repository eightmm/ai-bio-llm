# Problem
Problem 2. [Predicting Cellular Aging and Disease Progression from TERRA Modification Patterns]

TERRA (Telomeric Repeat-containing RNA) plays a critical role in regulating telomere length, replication stability, and genome integrity. Emerging evidence suggests that specific RNA modification patterns—such as m6A, pseudouridine, and m5C—on TERRA molecules may serve as early molecular indicators of cellular aging and disease progression.
In this problem, you are asked to design an AI-based multimodal predictive framework that infers cellular aging status or disease progression probability from TERRA modification patterns, and explicitly visualizes the functional contribution of individual RNA modifications.

Objective
Design an integrated predictive framework that:
1.	Predicts cellular aging or disease progression probability from TERRA-related multi-omics data
2.	Quantifies and visualizes the functional contribution of individual RNA modification types
3.	Provides mechanistic interpretability linking molecular features to biological pathways

Task
You must propose a framework that includes all three modules (A–C) described below.
For each module, clearly specify:
1.	Data types and AI tools used, with scientific justification
2.	Alternative design choices and why they were not selected
3.	How the module’s outputs feed into the next stage

Module A: Multimodal Data Integration and Normalization
Integrate heterogeneous data sources related to TERRA biology into a unified representation.

Data Modalities (examples)
•	Transcriptomic data
o	Total RNA-seq
o	Single-cell RNA-seq
•	Epigenetic / RNA modification data
o	RNA modification maps (e.g., m6A, Ψ, m5C)
o	MeRIP-seq / miCLIP
o	ChIP-seq related to telomere-associated factors
•	Imaging data
o	Telomere FISH
o	TERRA RNA-FISH
o	Super-resolution imaging (STED / SIM)-derived telomere morphology metrics

Module B: Predictive Model Design
Design a model that transforms the integrated multimodal representation into aging or disease probability predictions.

Module C: Model Interpretation and Evaluation
Explain how model predictions are interpreted and evaluated. 


---

# Prior research / evidence summary (Search output)
# An Integrated AI Framework for Predicting Cellular Aging and Disease Progression from TERRA Modification Patterns

This report presents a comprehensive scientific framework for designing a multimodal artificial intelligence system that predicts cellular aging status and disease progression probability from TERRA (Telomeric Repeat-containing RNA) modification patterns. The framework integrates heterogeneous data sources including transcriptomic profiles, RNA modification maps, and telomeric imaging data through an interpretable multimodal deep learning architecture. Central to the approach is the explicit decomposition of contributions from individual RNA modification types (N6-methyladenosine, pseudouridine, and 5-methylcytidine) and their mechanistic links to cellular aging pathways through pathway analysis and attention-based attribution. The design prioritizes both predictive accuracy and biological interpretability, incorporating uncertainty quantification, robustness to missing modalities, and validation against known telomere biology mechanisms to ensure clinical relevance and scientific credibility.

## Background: TERRA Biology and RNA Modifications as Biomarkers of Cellular Aging

TERRA represents a class of long non-coding RNA molecules transcribed from subtelomeric regions toward telomeres, producing heterogeneous transcripts ranging from approximately 100 base pairs to 9 kilobases in length[1]. These molecules exhibit a remarkable capacity to form R-loops—three-stranded nucleic acid structures consisting of RNA-DNA hybrids with displaced DNA strands—at telomeric regions, thereby exerting dual effects on telomere stability that have earned them the metaphorical description of a "double-edged sword"[1]. The functional role of TERRA extends beyond simple telomeric regulation; mounting evidence demonstrates that TERRA participates in chromosome-end protection and homologous recombination processes essential for the alternative lengthening of telomeres (ALT) pathway, a telomerase-independent mechanism of telomere maintenance active in approximately ten to fifteen percent of cancers[1][4].

The stability and functionality of TERRA molecules are themselves subject to post-transcriptional regulation through chemical modifications of ribonucleotides, collectively termed epitranscriptomics. Among these modifications, N6-methyladenosine (m6A) represents the most abundant internal modification in eukaryotic messenger and long non-coding RNAs[2]. Recent biochemical studies have revealed that m6A modification occurs specifically within the subtelomeric regions of TERRA and is catalyzed by the methyltransferase METTL3[1]. The methyltransferase METTL3-catalyzed m6A marks on TERRA are subsequently recognized and stabilized by the m6A reader protein YTHDC1, which prevents TERRA degradation[1]. Mechanistically, depletion of either METTL3 or YTHDC1 results in enhanced TERRA turnover, reduced formation of R-loops at telomeres, decreased homologous recombination activity, and ultimately telomere shortening and chromosomal instability[1]. This pathway has prompted consideration of METTL3 inhibition as a novel therapeutic strategy for ALT-dependent cancers, highlighting the clinical relevance of TERRA modification biology.

Beyond m6A, other RNA modification systems—including pseudouridylation (Ψ), 5-methylcytidine (m5C), 2'-O-ribose methylation (Nm), and 7-methylguanosine (m7G)—undergo age-dependent dysregulation with profound implications for cellular health[2][5]. In peripheral blood mononuclear cells from aged humans, total m6A modification levels demonstrate significant age-associated decline[2]. Similarly, in primates, skeletal muscle exhibits the greatest susceptibility to age-dependent loss of m6A modification across multiple tissues[2]. For m5C modifications, expression of the methyltransferase NSUN5 decreases during cellular senescence, contributing to organismal aging[2]. The m7G modification system shows comparable age sensitivity, with depletion of m7G-writing enzymes (METTL1 and WDR4) occurring during cellular senescence of human lung fibroblasts, and genetic inhibition of METTL1 accelerating senescence through persistent ribosome stalling[2]. Pseudouridine modification shows age-dependent increases in C. elegans, with approximately half of detected pseudouridine sites being unique to aged organisms and undergoing the highest age-dependent enrichment on mRNA molecules, thereby potentially affecting translation efficiency[2].

These age-dependent changes in RNA modification patterns suggest that epitranscriptomic dysregulation represents a core mechanism of cellular aging and may serve as a quantifiable biomarker of biological age. The integration of TERRA-specific modification patterns with genome-wide RNA modification maps, transcriptomic changes, and imaging phenotypes of telomere dysfunction could therefore provide a comprehensive biological signature of aging progression and disease susceptibility, motivating the development of an integrated predictive framework.

## Module A: Multimodal Data Integration and Normalization for TERRA Biology

### Modality-Specific Data Processing and Feature Engineering

The first critical step in the proposed framework involves the systematic preprocessing, quality control, and feature engineering of heterogeneous data modalities that collectively capture the molecular and cellular phenotypes associated with TERRA biology and aging. Each modality presents distinct technical and biological challenges requiring modality-specific protocols and tools while maintaining interoperability for downstream integration.

**Bulk RNA Sequencing Data Normalization and Batch Correction**

For bulk RNA-seq data, the recommended preprocessing pipeline begins with standard quality control filtering to remove low-quality libraries based on read depth, alignment rates, and distribution of mapped regions. Following quality filtering, raw read counts should be normalized using either transcripts per million (TPM) with log-transformation (log2(TPM+1)) or, preferably, the DESeq2 median ratio normalization approach, which computes sample-specific size factors that account for differences in sequencing depth and RNA composition biases that can confound downstream analyses[25]. The DESeq2 normalization is mathematically grounded in the assumption that most genes are not differentially expressed and represents a more principled approach than simple depth-based normalization. When datasets originate from multiple batches (different sequencing dates, library preparation protocols, or facilities), batch correction using ComBat-seq should be applied to remove systematic technical variation while preserving genuine biological differences between samples[8][25]. Importantly, the order of operations matters: pre-normalization should occur before batch correction to remove high-level technical variability, with post-normalization by library size applied after batch correction to ensure the final matrix reflects corrected counts while maintaining proper scale relationships[25].

**Single-Cell RNA-Seq Integration and Aggregation**

Single-cell RNA-seq data presents additional complexity due to the discrete, zero-inflated nature of transcript counts and the need to align cells across batches and studies. The recommended approach uses the scVI/totalVI framework, which employs a variational autoencoder architecture to jointly model the distribution of observed counts while accounting for batch effects and technical variables such as library size and mitochondrial transcript contamination[8]. These methods learn a low-dimensional latent representation for each cell while simultaneously performing denoising and batch correction, providing a unified embedding space from which cells can be mapped to biological states. Following scVI integration, sample-level aggregation can be achieved through one of several approaches: pseudo-bulk computation (summing counts within cell types or transcriptional states), attention-based pooling (learning to weight individual cells during aggregation), or direct use of cell-type proportion estimates combined with cell-type-level average expressions. For integration with bulk RNA-seq data, domain adaptation techniques can align the scVI latent embeddings to bulk samples by learning a linear or nonlinear mapping that bridges the two modalities, ensuring consistency across analysis stages.

**RNA Modification Mapping: Site-Level and Region-Level Feature Extraction**

RNA modification data derived from MeRIP-seq, miCLIP, or direct long-read sequencing presents the challenge of converting peak-based or coordinate-specific modification information into quantitative features suitable for machine learning prediction. The framework should support two complementary levels of feature representation. At the site level, individual modification peaks can be characterized by their genomic coordinates, peak intensity (enrichment fold as log2(IP/Input)), stoichiometry estimates (proportion of reads containing the modification), nucleotide context around the modification site (k-mers of flanking sequence), and structural context (predicted RNA secondary structure)[7][10]. Critically, the bias inherent in antibody-based methods such as MeRIP-seq must be corrected through statistical modeling of immunoprecipitation efficiency; the AEEIP tool provides a mixture model-based approach to estimate and correct for antibody bias across tissues and cell lines, improving accuracy in detecting m6A sites and reducing false negatives[10].

At the region level, features should be aggregated across TERRA-specific genomic regions (telomeric CCCTAA repeats, subtelomeric regions, and telomere-proximal genes) to capture the modification landscape relevant to TERRA biology. For each modification type (m6A, Ψ, m5C), the framework should compute: (1) mean modification density within telomeric and subtelomeric regions (number of peaks per kilobase), (2) modification peak intensity distributions (mean, median, variance), (3) co-occurrence patterns (correlation of modification presence across different types within shared regions), and (4) spatial clustering metrics (degree to which modifications cluster versus distribute uniformly). These features should be documented with explicit feature group labels indicating the modification type (m6A, pseudouridine, m5C) to enable later attribution analysis. Additionally, motif-context features can be derived by scanning flanking sequence windows around modification sites for known sequence motifs that predict modification positioning, such as DRACH motifs for m6A (D = adenine, guanine, or adenine; R = adenine or guanine)[7]. These motif-based features capture mechanistic information about site selection and can serve as validity checks on detected modifications.

**ChIP-Seq and Transcription Factor Occupancy Data**

ChIP-seq data for telomere-associated transcription factors and chromatin remodelers provide information about regulatory landscapes at telomeric regions. Following standard peak calling using tools like MACS2 with appropriate input normalization, peaks should be intersected with telomeric and subtelomeric coordinates to identify occupied regions. Features should quantify: (1) number of transcription factor binding sites in 5-kilobase windows centered on telomeres and TERRA promoters, (2) mean peak intensity (read depth) at binding sites, (3) co-occupancy patterns (which factors bind together), and (4) distance from telomeric repeats to nearest binding site. Importantly, because shelterin components (TRF1, TRF2, POT1) and chromatin organizing factors (CTCF, cohesin subunits such as Rad21) have been shown to bind TERRA promoters and control TERRA transcription, coverage at these specific loci should be prioritized[4]. Regional signals can be summarized as mean enrichment within genomic windows, enabling efficient integration with other modalities.

**Telomere Imaging Modalities: From Raw Image Data to Structured Features**

Telomeric and TERRA imaging data—derived from telomeric FISH, TERRA RNA-FISH, and super-resolution modalities such as STED and SIM—present unique opportunities to capture spatial and morphological information about telomeric structure and function. The processing pipeline should begin with image segmentation to identify individual telomeric foci, employing modern deep learning segmentation tools such as Cellpose (trained on nuclear and cytoplasmic membrane data) or StarDist (using radial distance maps for instance segmentation)[22]. Following segmentation, two complementary feature extraction strategies should be applied.

The first strategy involves handcrafted morphological features computed directly on segmented foci: telomere count per nucleus, total telomere intensity (sum of pixel values within foci), mean telomere intensity per focus, standard deviation of intensities (reflecting heterogeneity), spatial clustering metrics (degree of random versus clustered distribution computed via Ripley's K function), and colocalization metrics between FISH and RNA-FISH channels (Pearson or Manders correlation coefficients of signal overlap). For super-resolution imaging, additional metrics include sub-diffraction diameter estimates of foci (typically 130 nm spacing between distinct telomeres) and compactness measures (ratio of bounding box volume to actual segmented volume)[19][22]. These metrics directly correspond to known aspects of telomere biology: telomere count reflects chromosome number and chromosome stability, intensity reflects telomere length, heterogeneity (variance in intensity) suggests unequal replication or damage, spatial clustering indicates transcriptional activity or chromatin compaction, and colocalization of TERRA with telomeric DNA directly reflects TERRA-telomere association.

The second strategy leverages self-supervised deep learning to learn latent embeddings of telomeric morphology without requiring manual annotation. Vision Transformers (ViTs) pretrained on large image databases using self-supervised methods such as DINOv2 can be fine-tuned on telomeric FISH images to learn general-purpose visual features that capture subtle morphological variations relevant to aging and disease[45][48]. Alternatively, autoencoders can be trained on segmented telomeric foci to compress morphological information into low-dimensional bottleneck representations. The advantage of learned embeddings over handcrafted features is that they may detect informative morphological patterns not captured by predefined metrics. However, interpretability is traded for performance; the preferred approach combines both strategies by computing handcrafted features alongside learned embeddings and using feature importance analysis (Module C) to identify which learned embedding dimensions correlate with known telomere biology metrics, thereby grounding the learned representations in interpretable features.

### Multimodal Alignment and Integration Architecture

Following modality-specific preprocessing, the next critical step involves integrating heterogeneous modalities—transcriptomic profiles, RNA modification patterns, ChIP-seq signals, and imaging embeddings—into a unified representation suitable for downstream prediction. The integration strategy must balance several competing objectives: (1) preserving modality-specific information and allowing modality-specific encoders to specialize, (2) enabling the model to learn meaningful cross-modality relationships and interactions, (3) handling missing modalities gracefully (some samples may lack imaging data or certain RNA modification types), (4) maintaining interpretability by retaining correspondence between input features and learned representations, and (5) ensuring computational tractability at the scale of large cohorts.

**Multimodal Variational Autoencoders and Mixture-of-Experts Priors**

The recommended integration approach uses a multimodal Variational Autoencoder (VAE) architecture with a mixture-of-experts (MoE) data-dependent prior, a technique that has recently demonstrated superior performance to hard-constraint aggregation-based approaches[26][29]. In this architecture, each modality is processed by a specialized encoder neural network that maps raw features or learned embeddings to a modality-specific latent distribution \( q_{\phi}^{m}(z_m | x_m) \), where \( z_m \) represents the latent code for modality \( m \) and \( x_m \) denotes modality-specific input[26]. Rather than forcing these latent distributions to collapse to a shared representation through hard constraints (such as concatenation or averaging), the MoE approach defines a data-dependent prior distribution as a mixture over the modality-specific posteriors: \( p(z | X) = \sum_m w_m(X) \mathcal{N}(\mu_m, \sigma_m^2) \), where weights \( w_m(X) \) are learned functions of the full multimodal input and sum to one[26]. During the variational inference step, each modality's latent code is encouraged to match this mixture prior through a KL divergence term, inducing soft information sharing between modalities while allowing each to preserve specialized information[26].

This architecture offers several critical advantages over alternatives. Unlike early fusion (concatenation of raw features), it handles scale and dimensionality differences gracefully through modality-specific encoders. Unlike late fusion (training separate models on each modality and combining predictions), it enables cross-modality learning during the encoding phase. Unlike hard-constraint multimodal VAEs (e.g., product-of-experts), the soft MoE prior avoids the information loss and reconstruction degradation observed when multiple modalities are forced into a single bottleneck. Empirical comparisons demonstrate that the MoE-prior VAE achieves both superior reconstruction fidelity (better generative coherence) and more informative latent representations (better performance on downstream tasks) compared to existing multimodal VAE approaches[26].

**Missing Modality Robustness Through Probabilistic Masking**

A critical practical consideration in clinical and research settings is that some samples may lack complete modality information due to sample quality issues, technical limitations, or practical constraints (e.g., imaging may be unavailable for some cell lines). The MoE-VAE framework naturally handles missing modalities by learning the prior weights conditioned on which modalities are observed. During training, modalities are randomly dropped (modality dropout), forcing the model to learn how to perform inference when subsets of modalities are available[26][29][30]. At inference time, the modality mask is provided to the model, allowing the prior to adapt accordingly. For data imputation of missing modalities, the learned latent representation from observed modalities can be used to generate synthetic features for missing modalities via conditional sampling from the generative model, enabling downstream analysis without discarding incomplete samples.

**Explicit Modification-Type Feature Grouping for Attribution**

To enable interpretable attribution of predictions to individual RNA modification types (m6A, pseudouridine, m5C), the integration architecture must maintain explicit groupings of features by modification type throughout the integration pipeline. Specifically, RNA modification features should be organized into three feature groups corresponding to the three modification types. When these features are encoded by the respective modality encoder, the encoder should be structured to output a separate latent code vector for each modification type: \( z_{m6A}, z_\Psi, z_{m5C} \), rather than a single mixed latent code[26][29]. These modification-type-specific codes can then be independently embedded and processed before being aggregated into the overall multimodal latent representation. This structured approach enables later attribution analysis to directly assign prediction changes to individual modification types without the confounding effects of unrelated feature interactions.

**Contrastive Alignment Between Modalities**

In addition to the generative modeling objective of the VAE, an auxiliary contrastive learning objective can improve multimodal alignment and ensure that corresponding information across modalities is aligned in the learned latent space[26][38]. For example, samples known to have strong biological relationships (e.g., the same cell type from different modalities) should have similar latent representations, while unrelated samples should diverge. A contrastive loss such as InfoNCE (used in contrastive learning frameworks) or normalized softmax triplet loss can be combined with the VAE objective to encourage this alignment without sacrificing the flexibility of the mixture-of-experts architecture[26][29].

### Output Interface and Specifications for Module B

The multimodal integration module (Module A) produces three complementary output representations, each formatted with explicit documentation to enable downstream attribution analysis:

1. **Unified Sample-Level Embedding**: A fixed-dimensional vector \( z_{unified} \) representing the integrated information from all available modalities. This embedding should be dimensionally consistent (e.g., 256 or 512 dimensions) across all samples and should be calibrated such that Euclidean distance roughly corresponds to biological similarity (age distance, disease state similarity). Metadata should document the dimensionality and whether the embedding has been standardized (zero mean, unit variance).

2. **Modality-Specific Embeddings with Alignment Masks**: The individual latent codes \( z_{omics}, z_{imaging}, z_{modification} \) corresponding to major data modalities (omics, imaging, RNA modifications) should be provided as separate tensors. For RNA modifications, further separation into \( z_{m6A}, z_\Psi, z_{m5C} \) enables modification-type-specific attribution. A modality presence mask should document which modalities are available for each sample (binary vector indicating present/absent status).

3. **Feature Attribution Metadata**: A structured document mapping learned latent dimensions back to input feature groups by modality and modification type. This enables Module C attribution methods to be informed about the input structure, improving interpretability. For example, the documentation should specify which input features contributed to which latent dimensions, allowing attribution of latent dimensions to be traced back to original RNA modifications or imaging metrics.

## Module B: Multimodal Predictive Model Design and Architecture

### Task Formulation and Target Variable Selection

The selection of an appropriate prediction task and target variable definition is fundamental to model design and ultimately determines the interpretability and clinical utility of the predictions. The framework should support multiple task formulations to accommodate different study designs and research questions:

**Aging Stage Classification**

If the aging process is operationalized as discrete stages (e.g., young, intermediate, aged, and senescent), the task becomes ordinal multiclass classification. Ordinal regression—which explicitly models the ordering relationship between classes—should be preferred over standard multiclass classification because aging stages naturally form a rank-ordered continuum[37][40]. Ordinal regression methods transform the problem into a series of binary classification tasks ("is the sample at least stage k?") for each stage k, allowing the model to learn that predictions for adjacent stages should be similar while respecting the overall ordering[37][40]. This improves both prediction accuracy and model stability compared to treating stages as unordered categories.

**Continuous Biological Age Prediction**

If age is treated as a continuous variable (biological age derived from aging clocks or chronological age), standard regression is appropriate with a calibration objective to ensure that predicted ages roughly match observed ages across the population. Mean absolute error (MAE) or mean squared error (MSE) can serve as the primary loss, with additional calibration metrics (e.g., expected calibration error or reliability curves) computed to verify that confidence intervals around predictions are accurate.

**Time-to-Disease Progression Survival Modeling**

For prospective studies in which some individuals develop disease while others remain healthy during follow-up, survival analysis formulations are appropriate. The Cox proportional hazards model has long served as a standard in medical statistics, but neural network extensions—such as DeepSurv, which replaces the linear Cox risk score with a deep neural network—enable modeling of complex nonlinear relationships between features and hazard rates[15][18]. For time-to-event prediction, the model outputs a survival function \( S(t | x) \) or cumulative hazard function \( H(t | x) \) given input features, and evaluation metrics include the concordance index (C-index), which measures ranking agreement between predicted and observed survival times, and time-dependent AUC, which evaluates discrimination performance at fixed time horizons[15][18].

### Multimodal Transformer with Modification-Type Gating

The recommended model architecture for Module B combines several recent advances in deep learning: transformer-based cross-modality attention, mixture-of-experts routing, and structured prediction heads. The overall architecture consists of three stages: (1) multimodal encoding via pre-trained or jointly-trained encoders, (2) multimodal fusion via cross-attention and modification-type gating, and (3) prediction via task-specific heads with uncertainty quantification.

**Multimodal Transformer Encoder with Cross-Attention Fusion**

The first stage begins with the outputs from Module A (modality-specific embeddings) as input. Each modality embedding is projected into a high-dimensional feature space and then passed through a learned positional encoding and embedding layer to produce token sequences. For transcriptomic data, tokens can represent gene sets or pathway-level summaries (pathway activity scores from PROGENy or GSVA) to reduce dimensionality and improve interpretability. For RNA modification data, tokens represent modification-type groups (m6A, pseudouridine, m5C) or individual peak regions. For imaging data, tokens can represent learned patch-level features or morphology metric groups.

These token sequences from different modalities are then fused via a cross-attention transformer architecture, in which tokens from one modality serve as queries while tokens from another modality serve as keys and values[20][23]. For example, imaging tokens (queries) can attend to transcriptomic tokens (keys/values), allowing the model to learn which gene expression patterns are associated with specific morphological features. Cross-attention fusion is preferred over simple concatenation or averaging because it preserves modality-specific structure while learning task-relevant alignments without requiring explicit paired training data.

**Modification-Type Gating via Mixture of Experts**

To enable direct attribution of predictions to individual RNA modification types, the architecture incorporates a modification-type routing layer structured as a sparse mixture-of-experts (MoE) module[31][34]. The MoE module contains three expert sub-networks, one per modification type (m6A, pseudouridine, m5C), plus a gating network that learns to assign each input sample to the most relevant expert(s). The gating network outputs a softmax weight vector \( w(x) = [w_{m6A}(x), w_\Psi(x), w_{m5C}(x)] \) that sums to one, and the final representation is a weighted combination of expert outputs: \( y_{MoE} = \sum_m w_m(x) \cdot f_m(x) \), where \( f_m(x) \) is the output of expert m[31][34]. During training, an auxiliary load-balancing loss encourages each expert to be utilized approximately equally across the training set, preventing collapse toward a single expert[31][34].

The advantages of this MoE design are substantial: (1) gating weights directly quantify the importance of each modification type for each sample, (2) experts can specialize to learn modification-type-specific patterns, (3) the sparse routing (only active experts are computed) maintains computational efficiency, and (4) gating weights provide a natural basis for Module C attribution analysis. Alternative approaches—such as concatenating modification-type features and relying on feature importance analysis—lack these direct mechanistic connections.

**Task-Specific Prediction Heads with Uncertainty Quantification**

The learned multimodal representation (post-MoE fusion) is fed into task-specific prediction heads. For classification tasks (ordinal aging stages, disease status), a softmax prediction head outputs class probabilities alongside uncertainty estimates derived from deep ensembles (training multiple instances of the model with different random initializations) or Monte Carlo dropout (performing multiple stochastic forward passes through dropout layers at inference time)[32][35][50]. For regression tasks (continuous age prediction), a prediction head outputs both mean and variance estimates, enabling calibrated prediction intervals. For survival tasks, a DeepSurv-style head outputs the log-hazard (or relative risk) at each time point, from which survival curves can be computed.

Uncertainty quantification is critical for clinical translation, as predictions accompanied by calibrated confidence intervals enable clinicians to distinguish high-confidence predictions (that should guide clinical decisions) from uncertain predictions (that require additional investigation)[35]. The CLUE (Calibration via Learning Uncertainty-Error Alignment) approach provides a scalable method to explicitly align predicted uncertainty with observed prediction error during training, improving both calibration and aleatoric uncertainty estimation[32]. Deep ensembles augmented with Bayesian optimization (BODE) further improve uncertainty estimation while reducing uncertainty overestimation compared to manually-tuned baselines[50].

### Training Objectives and Loss Functions

**Primary Supervised Loss**

The primary loss function depends on the task formulation. For ordinal regression, ordinal cross-entropy loss is preferred, which treats each binary sub-problem independently and sums losses across binary classifiers[37][40]. For regression, mean squared error or mean absolute error is appropriate, potentially with a heteroscedastic term modeling input-dependent prediction variance[35]. For survival analysis, the partial likelihood of the Cox model or a discrete-time hazard loss can be used[15][18].

**Auxiliary Objectives for Robustness and Interpretability**

Beyond the primary supervised loss, several auxiliary objectives improve model robustness and support interpretability. A contrastive alignment loss between modalities encourages consistency in the learned representations for samples with similar biological characteristics[26][38]. A sparsity or group-lasso penalty on the MoE gating weights can encourage the model to rely on a small number of modification types per sample, improving interpretability by preventing distributed responsibility[31]. An ensemble diversity loss, when training multiple models, encourages ensemble members to make different predictions on hard samples while agreeing on easy samples, improving both accuracy and uncertainty quantification[50].

**Handling Class Imbalance in Aging/Disease Prediction**

When the aging population is imbalanced (e.g., few truly senescent cells), standard cross-entropy loss can lead to biased predictions that underestimate the probability of rare classes. Focal loss, which down-weights easy negative examples and focuses training on hard examples, can improve rare-class prediction[32]. Alternatively, class weights can be incorporated into the loss function to penalize misclassification of rare classes more heavily. For ordinal regression with imbalance, stratified sampling during training ensures that each binary sub-problem receives balanced class representation[42].

### Outputs to Module C: Prediction Representations and Intermediate Signals

The trained Module B model outputs predictions alongside intermediate representations suitable for downstream interpretation:

1. **Predicted Probabilities and Risk Scores**: Final predictions including class probabilities (for classification), predicted age/biomarker values with prediction intervals (for regression), or survival probabilities at specific time horizons (for survival models). Risk scores should be calibrated to ensure that predicted probabilities match empirical frequencies.

2. **Modification-Type Gating Weights**: The softmax gating weights \( w_{m6A}, w_\Psi, w_{m5C} \) output by the MoE routing layer for each sample. These directly indicate the learned importance of each modification type for that sample's prediction.

3. **Expert Activations**: The pre-softmax expert outputs \( f_m(x) \) for each modification type, capturing the modification-specific signal before aggregation. These can be used for modification-specific sensitivity analysis.

4. **Attention Weights from Cross-Modality Layers**: Attention weight matrices from transformer cross-attention layers, documenting which tokens from one modality attend to which tokens from another modality. These quantify learned cross-modality relationships.

5. **Latent Representations**: Learned embeddings at intermediate layers of the neural network, particularly the pre-prediction-head representation, suitable for downstream pathway association analyses and dimensionality visualization.

## Module C: Model Interpretation, Attribution, and Evaluation with Mechanistic Links

### Quantitative Attribution of Individual RNA Modification Types

Module C addresses the critical challenge of transforming black-box neural network predictions into biologically interpretable, actionable insights about which RNA modifications and biological pathways drive aging and disease progression predictions.

**Integrated Gradients and SHAP for Modification-Type Attribution**

The primary approach for attribution leverages Integrated Gradients (IG), a gradient-based feature attribution method that measures the contribution of each input feature to a model's prediction by integrating the gradient of model output with respect to inputs along a path from a baseline (typically zero or a neutral reference) to the actual input[16][58]. Mathematically, IG computes: \( IG_i(x) = (x_i - x'_i) \int_0^1 \frac{\partial f(x' + \alpha(x - x'))}{\partial x_i} d\alpha \), where \( f \) is the model, \( x \) is the actual input, \( x' \) is the baseline, and the integral is approximated numerically[16]. IG satisfies crucial axioms including completeness (attributions sum to the difference between model output at input and baseline) and sensitivity (features that change the output are identified as non-zero attribution)[16].

For the modification-type-structured model described in Module B, IG should be computed at two levels: (1) attribution to individual modification-type groups (m6A, pseudouridine, m5C) by aggregating gradients across all features within each group, and (2) attribution to specific peaks or regions within each modification type by computing fine-grained gradients. The aggregation step—summing gradients within feature groups—is mathematically valid because gradients are additive and localizing gradients to feature groups preserves the interpretation that the sum represents the total contribution of that group[16]. This modification-type-level attribution directly answers the research question "which modifications most influence aging predictions?"

For comparison and robustness, SHAP (SHapley Additive exPlanations), a game-theoretic feature importance method based on Shapley values, should also be computed[13]. SHAP provides complementary insights: while IG is computationally efficient and gradient-based (fast for differentiable models), SHAP provides stronger theoretical guarantees of fairness and consistency across feature groups[13]. The key distinction is that SHAP computes the expected change in prediction when a feature is marginalized out (absent from the model), whereas IG computes derivatives along a specific path from baseline to input. For highly correlated features (which is common in biological data), the two methods can diverge; examining agreement between IG and SHAP highlights robust versus fragile attributions[13].

**Modification-Specific Sensitivity Analysis Through Counterfactual Masking**

Beyond gradient-based attribution, causal-style sensitivity analysis provides complementary mechanistic insights. This involves systematically masking or zeroing out feature groups for individual modification types and measuring the resulting change in predicted risk or aging probability (delta-risk or causal sensitivity)[33]. For example, setting all m6A-related features to zero and re-running the model reveals the model's learned dependence on m6A signals. This counterfactual approach is intuitive and directly interpretable: if masking m6A features causes a 0.3 unit decrease in predicted aging probability, m6A modifications are clearly relevant to the model's aging assessment.

The advantage over gradient-based methods is that counterfactual masking directly measures model behavior under explicit input changes, without relying on local gradient approximations. A disadvantage is computational cost: each feature group requires a separate forward pass. The permutation importance method, which randomly shuffles features and measures performance degradation, represents a model-agnostic variant of this approach[33][36]. Permutation importance is particularly valuable for non-differentiable models and provides calibrated importance rankings on held-out test sets, avoiding overfitting bias from training data.

**Visualization of Modification-Type Contributions and Site-Specific Heatmaps**

Attribution analysis results should be visualized in multiple complementary formats to support hypothesis generation and biological interpretation. Contribution bars or violin plots should display the distribution of attribution magnitudes across samples, stratified by aging stage or disease status, revealing which modification types are consistently important versus sample-dependent. Site-specific heatmaps should display modification-level (per-peak or per-region) attributions in a matrix format with genomic coordinates on the x-axis (ordered by chromosomal location or grouped by telomere versus subtelomeric regions) and samples on the y-axis, sorted by predicted aging or disease progression. Color intensity indicates attribution magnitude (absolute value), and directionality (red for increasing risk, blue for decreasing risk) conveys the sign of attributions. These heatmaps rapidly communicate which genomic regions and which samples show modification-specific patterns driving predictions.

### Cross-Modality Interpretability Through Attention Rollout and Modality Ablation

**Attention Rollout for Cross-Modality Tracing**

In the multimodal transformer architecture of Module B, cross-attention weights document which tokens from one modality attend to which tokens from another modality. These attention weights can be "rolled out" (propagated through successive attention layers) to compute end-to-end attention flow from input modalities to output predictions[55][58]. The attention rollout method computes the cumulative attention from layer L to layer 1 as: \( AttentionRollout_L = (A_L + I) \cdot AttentionRollout_{L-1} \), where \( A_L \) is the attention matrix at layer L, \( I \) is the identity matrix (accounting for residual connections), and the operation is normalized at each step[55][58].

For multimodal prediction, attention rollout reveals which imaging regions (e.g., specific telomeric foci) attend to which transcriptomic signals (e.g., specific genes or pathways), visualizing learned structure-function relationships. A practical refinement is to discard low-attention values and focus visualization on high-attention pathways, improving signal-to-noise ratio[58]. This analysis answers questions like "do cells with concentrated (non-diffuse) telomeric foci selectively attend to expression patterns associated with DNA damage response?"

**Modality Ablation Curves and Permutation Importance by Modality**

Modality ablation involves systematically removing each modality and evaluating prediction performance degradation. Unlike feature-level ablation, modality-level ablation measures the aggregate importance of entire modalities and is computationally tractable even for high-dimensional modalities. Permutation importance extended to modalities randomly shuffles all features within a modality and measures performance degradation[36]. The resulting permutation importance scores quantify the predictive value of each modality: imaging might contribute X% to overall predictive performance, RNA modifications Y%, and transcriptomics Z%. These curves are particularly valuable because they address the practical question "in a resource-constrained setting, which modalities should be prioritized?" If imaging provides marginal gains after incorporating RNA modifications and transcriptomics, it may be deprioritized in clinical deployment.

Modality ablation can be further refined by ablating modification-type-specific features (all m6A features, all pseudouridine features, etc.) to assess modification-type importance at a finer grain than individual feature attribution. This provides a bridge between granular feature attribution (Module C) and overall model architecture (Module B).

### Mechanistic Interpretability Through Pathway Association and Concept Bottleneck Models

**Gene Set Enrichment Analysis Linking RNA Modifications to Expression Programs**

While attribution methods directly identify which modifications influence predictions, mechanistic interpretability requires establishing the biological basis for these associations. This is achieved by relating attributed modification signals to downstream transcriptomic changes and known biological pathways. The recommended approach is to correlate modification-type attribution scores with gene set enrichment scores computed via GSVA or ssGSEA, which estimate the coordinated activity of gene sets (pathways, hallmarks, cell-type signatures) within each sample[14][17]. For example, if m6A modifications show high attribution for predicting aging, one hypothesizes that m6A changes drive alterations in translation efficiency or mRNA stability, leading to coordinated changes in gene expression. By correlating per-sample m6A attribution with per-sample enrichment scores for "mRNA translation" or "ribosome biogenesis" pathways, this hypothesis can be tested empirically.

GSEA (Gene Set Enrichment Analysis), a rank-based approach, extends this by determining whether modifications associated with high aging are enriched at the top or bottom of gene expression rank-ordered lists[14][17]. If samples with high m6A attribution tend to have upregulated "DNA damage response" and "senescence" pathways, this would suggest that m6A dysregulation drives a DDR/senescence transcriptional program—a mechanistically interpretable finding[14][17].

**Transcription Factor Activity Inference Linking to Telomere Maintenance**

Beyond gene sets, transcription factor (TF) activity inference methods such as DoRothEA or PROGENy estimate the activity of individual transcription factors or pathway components from gene expression patterns[44][56]. For TERRA biology, this is particularly relevant because telomere maintenance depends on the coordinated activity of telomere-protective factors (TRF1, TRF2, POT1) and TERRA regulatory factors (NRF1, which upregulates TERRA under oxidative stress)[4]. By inferring TF activities from expression data and correlating them with modification attributions, one can test whether m6A modifications on TERRA associate with altered activities of TERRA-regulating TFs. For example, if METTL3-catalyzed m6A on TERRA stabilizes TERRA transcripts, one would predict that samples with high m6A attribution show elevated NRF1 activity or TERRA expression levels; DoRothEA-inferred NRF1 activity can be directly correlated with m6A attribution scores[44][56].

**Concept Bottleneck Models for Intermediate Biological Concepts**

A recent approach to mechanistic interpretability employs Concept Bottleneck Models (CBMs), which predict intermediate human-interpretable biological concepts (e.g., "DNA damage activation," "replication stress," "telomere dysfunction-induced foci") as hidden layers, then use these concepts to predict the final outcome[24]. For TERRA aging prediction, intermediate concepts might include:

1. Telomere Dysfunction Level: degree of persistent DNA damage response at telomeres, estimated from γH2AX foci counts and persistence.
2. TERRA Expression Abnormality: deviation from age-appropriate TERRA expression levels.
3. R-Loop Stability: balance between TERRA-mediated R-loop formation and resolution, estimated from TERRA and RNase H expression patterns.
4. Replication Stress Activation: heightened checkpoint activation and dNTP depletion, estimated from gene expression of replication stress markers.
5. Telomere-Induced Senescence: commitment to the senescent cell fate, estimated from p21 and p16 expression or cell-cycle arrest gene sets.

By training the model to first predict these concepts from multimodal inputs, then predict aging from concepts, the model's decision logic becomes transparent: "the model predicts high aging probability because this sample shows high telomere dysfunction + replication stress + elevated TERRA expression changes." This decomposition aids hypothesis generation (which concept should clinicians focus on?) and model debugging (do concept predictions match experimental measurements?).

### Rigorous Evaluation Protocols and Robustness Assessment

**Stratified Cross-Validation with Batch-Aware Splitting**

Standard k-fold cross-validation can be misleading when data contains batch effects (samples from different studies, sequencing dates, or preparations). Stratified cross-validation ensures that class distributions (aging stages) are preserved across folds, preventing fold imbalance from confounding metrics[42]. Additionally, batch-aware splitting ensures that no batch appears in both training and test sets for the same fold, providing a realistic assessment of generalization to new batches. For survival/time-to-event analyses, stratification should account for event status (censoring), ensuring that both event and censored samples are balanced across folds[42].

**Calibration Assessment and Uncertainty Evaluation**

Beyond accuracy metrics (AUROC, AUPRC for classification; MAE for regression), calibration must be rigorously evaluated. Expected Calibration Error (ECE) measures the mean absolute difference between predicted probabilities and empirical frequencies, with calibration considered good when ECE < 0.1[32][35]. Reliability plots (binning predictions into deciles and comparing predicted vs. empirical probabilities within each bin) provide visual assessment of calibration[32]. For regression, similar calibration checks examine whether 95% of observed values fall within the predicted 95% confidence intervals (coverage should equal 0.95)[35].

**Robustness Under Missing Modalities**

To validate that the framework's missing-modality handling mechanisms are effective, systematic robustness tests should evaluate prediction performance when different combinations of modalities are withheld at test time. If imaging data was removed for 10% of samples during training (with modality dropout), test performance should gracefully degrade rather than collapse when imaging is unavailable at inference. AUROC or MAE with imaging removed should be minimally lower than with imaging present, indicating that the model has learned to leverage other modalities as backups[26][29][30].

**Distribution Shift and Out-of-Distribution Detection**

Real-world deployment risks encountering distribution shifts: samples from populations not represented in training data (e.g., different cell types, donor ages, disease states). Robustness can be evaluated by artificially inducing distribution shifts during evaluation (e.g., holding out a specific age group or disease state during training) and assessing performance degradation[57]. Additionally, out-of-distribution (OOD) detection methods can flag samples whose feature distributions diverge substantially from training data, enabling clinicians to request additional investigation for flagged samples[35]. Ensemble disagreement (variance in predictions across ensemble members) serves as a natural OOD score.

**Permutation Tests for Attribution Stability**

Attribution methods (IG, SHAP) are sensitive to model randomness and data perturbations. To assess whether attribution rankings are stable, permutation tests can repeatedly train the model with different random seeds and recompute attributions, comparing rankings across runs. Stable attributions (high rank correlation across runs) represent robust findings, while unstable attributions suggest they are artifacts of the specific model fit and should be interpreted cautiously[33].

**Biological Plausibility Validation Against Known Telomere Biology**

Finally, interpretation results should be validated against established biology. If modification-type attribution analysis identifies m6A as highly predictive of aging, one would expect: (1) METTL3 expression to correlate with aging outcomes (from known biology), (2) m6A-attributed modifications to be enriched at TERRA and known telomere maintenance genes, (3) samples with high m6A attribution to show upregulated expression of TERRA target genes, and (4) perturbation studies (METTL3 knockdown) to phenocopy predictions made for low-m6A samples. Systematic alignment with known biology serves as a ground-truth check on interpretation validity.

## Integration and Workflow: End-to-End Prediction and Interpretation

The three modules form an integrated pipeline in which outputs from each stage feed into subsequent stages, enabling end-to-end aging prediction with detailed mechanistic attribution. The workflow proceeds as follows:

An input sample (cells, tissue, or patient cohort) arrives with multimodal data: RNA-seq, RNA modification maps (MeRIP-seq, miCLIP), ChIP-seq, and imaging (FISH, super-resolution). Module A ingests these data, applies modality-specific quality control and preprocessing (Section III, "Modality-Specific Data Processing"), computes features with explicit modification-type groupings, and integrates them via the multimodal VAE with MoE prior (Section III, "Multimodal Alignment"). The outputs are a unified sample embedding and modification-type-specific embeddings.

These embeddings are passed to Module B, the predictive model, which combines a multimodal transformer with cross-attention fusion and a modification-type MoE gating layer (Section IV). The model outputs predicted aging probability (or age, disease progression risk, depending on task), alongside gating weights per modification type, expert activations, and cross-attention weights. Uncertainty estimates are derived from deep ensembles or MC dropout.

Module C takes the predictions and intermediate model signals and performs detailed interpretation. Integrated Gradients and SHAP are computed to attribute prediction changes to individual modification types and genomic regions. Counterfactual masking reveals the model's dependence structure. Cross-modality attention is visualized to show learned structure-function relationships. Pathway enrichment and TF activity inference link attributed modifications to downstream biology. Concept bottleneck predictions (if trained) provide interpretable intermediate concepts explaining the prediction. Finally, the model is evaluated on held-out test sets with stratified CV, calibration assessment, missing-modality robustness tests, and comparison against biological priors.

The outputs of Module C—attribution plots, pathway associations, concept explanations, and calibration curves—provide clinicians and researchers with actionable insights: which modifications drive aging for this sample, through which biological pathways, with what confidence. This end-to-end workflow balances predictive performance, mechanistic interpretability, and clinical utility.

## Implementation Considerations and Technical Challenges

### Handling High-Dimensional Omics Data with Limited Sample Sizes

A recurring challenge in biomedical research is that sample sizes (number of patients or cell lines) are often small (tens to hundreds) relative to feature dimensions (tens of thousands of genes, thousands of modification sites). This curse of dimensionality can lead to overfitting and poor generalization. Several strategies mitigate this challenge. Feature selection or dimensionality reduction (PCA, VAE compression) prior to model training reduces feature count. Transfer learning, using models pretrained on large public databases (GTEx, ENCODE, GEO), provides learned features that generalize better than random initialization[38][41]. Regularization techniques—including L1/L2 penalties, dropout, and batch normalization—reduce model capacity and improve generalization. Semi-supervised learning, leveraging unlabeled samples (which are more abundant than labeled samples), can improve learning efficiency when explicit aging labels are scarce[26][29]. Data augmentation, while common in imaging, is less developed for tabular omics data but can be approached through careful synthetic data generation conditioned on biological constraints.

### Computational Scalability for Whole-Genome Integration

Full genome-wide analysis (integrating expression and modifications across all ~20,000 human genes) is computationally expensive. Practical implementations should focus on telomere-relevant subsets: telomeric repeat regions, TERRA source regions, telomere maintenance genes (TERF1, TERF2, POT1, TINF2, RAP1), senescence-associated genes (CDKN1A, CDKN2A), DNA damage response genes (TP53, ATM, ATR), and replication stress markers. This "focused genome" approach reduces dimensionality, improves interpretability, and accelerates computation while preserving biological relevance. For high-dimensional modalities (whole-genome modification maps), pathway-level or region-level aggregation reduces features before model training.

### Batch Effect Correction: Order of Operations and Validation

Batch effect correction is critical but risky: aggressive correction can remove true biological signals. The recommended order is (1) per-modality quality control and normalization, (2) modality-specific batch correction (ComBat-seq for RNA-seq, internal scVI batch correction for scRNA-seq), (3) multimodal integration, (4) inspection of batch correction via PCA or UMAP visualization. Post-integration batch correction should be avoided if possible, as it can confound the integrated representation. Additionally, batch should be included as a covariate in differential expression or pathway analysis, allowing batch effects to be statistically adjusted without removing them, which maintains the association between batch and outcome if such associations exist.

### Missing Data and Modality Dropout Strategies

In real-world data, modalities are often missing for specific samples due to sample quality, practical constraints, or study design. The multimodal VAE with modality dropout (Section IV) handles this by training the model to perform inference when subsets of modalities are available. During training, each modality is randomly dropped with a certain probability (e.g., 20%), forcing the model to learn to integrate information from all modality combinations. At inference, the actual modality presence mask is provided, allowing the prior to adapt. This approach is theoretically principled and empirically effective, but requires sufficient training data to encounter all modality combinations with non-negligible frequency; for studies with very sparse modality combinations, explicit imputation or submodal-specific models may be necessary.

## Conclusion

The proposed framework for predicting cellular aging and disease progression from TERRA modification patterns represents a synthesis of recent advances in multimodal machine learning, biological pathway analysis, and mechanistic interpretability. By integrating heterogeneous data modalities—transcriptomic profiles, RNA modification maps, chromatin immunoprecipitation data, and telomere imaging—through a principled multimodal VAE architecture, the framework enables sample-level predictions of aging state and disease risk. The explicit decomposition of contributions from individual RNA modification types (m6A, pseudouridine, m5C) through modification-type-specific gating and grouped attribution analysis directly addresses the core research question: which RNA modifications drive cellular aging?

The framework's emphasis on mechanistic interpretability—through pathway enrichment analysis, transcription factor activity inference, concept bottleneck models, and validation against known telomere biology—bridges the gap between black-box predictive accuracy and biological understanding. By requiring that interpretation results align with established mechanisms of telomere dysfunction and aging, the framework supports hypothesis generation and identifies targets for mechanistic validation.

Critical implementation choices include the use of multimodal VAEs with mixture-of-experts priors for robust cross-modality alignment and missing-data handling, transformer-based architectures with cross-attention fusion for capturing nonlinear modality interactions, and gradient-based attribution methods (Integrated Gradients, SHAP) complemented by counterfactual sensitivity analysis for granular feature importance. Rigorous evaluation through stratified cross-validation, calibration assessment, missing-modality robustness tests, and biological plausibility checks ensures that the framework produces reliable, trustworthy predictions suitable for clinical translation.

Future developments should focus on prospective validation in independent cohorts, integration of genetic variants and epigenetic marks (DNA methylation, chromatin accessibility) for deeper mechanistic insight, application to diverse cell types and tissue contexts (not only cultured cells), and development of causal inference methods to distinguish correlation from causation in modification-to-phenotype associations. As multimodal deep learning and interpretation methodologies continue to mature, the framework provides a template for translating high-dimensional molecular measurements into actionable biological insight.

---

# Data analysis summary (Data analysis output)
{
  "status": "skipped",
  "reason": "No sub_problem with DB_flag=1",
  "problem_id": "terra_modification_multimodal_aging_prediction"
}

---

# Writing Task
Using all information above, write the final answer that **exactly matches the output format and requirements requested in the Problem**.

Mandatory rules:
- If the problem lists numbered requirements (e.g., "(1)...(5)"), **preserve the numbering/structure** in your answer.
- If the problem asks for a strategy/pipeline/analysis plan, include **concrete steps, inputs, outputs, metrics, and caveats**.
- If data files are mentioned, describe **what to read and what to compute** (procedure/algorithm-level; not necessarily code).
- Explicitly state **assumptions/limitations** where uncertainty or missing information exists.
- **Write in English.**

**Output only the final answer text.**
